# Llama3 Inference Project for ML Engineers

For this project, I'd like you to research and implement the following:

1) small POC to achieve >300 tokens per second on Llama3 models and share your findings. 
2) how would you deploy a Llama3 model into AWS for production and what hardware would you use

[1] Speculative Edits: https://cursor.sh/blog/instant-apply
[2] https://x.com/amanrsanger/status/1790947733899203027
